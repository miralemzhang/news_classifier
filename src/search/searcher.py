"""
æ™ºèƒ½æœç´¢æ¨¡å—
æ ¹æ®ç”¨æˆ·æŸ¥è¯¢è‡ªåŠ¨å®šä½åˆ†ç±»ï¼Œä½¿ç”¨ LLM è¯»å–å†…å®¹ï¼Œè¿”å› Top K ç›¸å…³ç»“æœ

æµç¨‹ï¼š
1. ç”¨æˆ·è¾“å…¥æŸ¥è¯¢ â†’ å¦‚ "2020 ä¸­å›½ å¤±ä¸šäººå£"
2. è‡ªåŠ¨å®šä½åˆ†ç±» â†’ æ ¹æ®å…³é”®è¯åŒ¹é…åˆ° "æ—¶æ”¿" ç±»åˆ«
3. åœ¨è¯¥åˆ†ç±»çš„æ•°æ®ä¸­æœç´¢ç›¸å…³æ–‡æ¡£
4. ä½¿ç”¨ LLM æå–/æ€»ç»“ç›¸å…³ä¿¡æ¯
5. è¿”å› Top K ç»“æœç»™ç”¨æˆ·
"""

import json
import os
import re
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, field
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from rules.classifier import RuleClassifier


@dataclass
class SearchResult:
    """å•æ¡æœç´¢ç»“æœ"""
    rank: int                    # æ’å
    title: str                   # æ ‡é¢˜
    url: str                     # URL
    category: str                # åˆ†ç±»
    snippet: str                 # æ‘˜è¦ç‰‡æ®µ
    relevance_score: float       # ç›¸å…³æ€§åˆ†æ•°
    source_file: str             # æ¥æºæ–‡ä»¶
    matched_keywords: list = field(default_factory=list)  # åŒ¹é…çš„å…³é”®è¯


@dataclass 
class SearchResponse:
    """æœç´¢å“åº”"""
    query: str                   # åŸå§‹æŸ¥è¯¢
    detected_category: str       # æ£€æµ‹åˆ°çš„åˆ†ç±»
    total_found: int             # æ‰¾åˆ°çš„æ€»æ•°
    results: list                # ç»“æœåˆ—è¡¨ (Top K)
    llm_summary: str = ""        # LLM æ€»ç»“ï¼ˆå¯é€‰ï¼‰


class SmartSearcher:
    """æ™ºèƒ½æœç´¢å™¨"""
    
    def __init__(
        self,
        data_dir: str = None,
        keywords_path: str = None,
        use_llm: bool = True,
        llm_model: str = "qwen2:7b",
        top_k: int = 15
    ):
        """
        åˆå§‹åŒ–æœç´¢å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•ï¼ˆåŒ…å«åˆ†ç±»åçš„ç½‘é¡µæ•°æ®ï¼‰
            keywords_path: å…³é”®è¯åº“è·¯å¾„
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM è¿›è¡Œå†…å®¹æå–å’Œæ€»ç»“
            llm_model: LLM æ¨¡å‹åç§°
            top_k: è¿”å›ç»“æœæ•°é‡
        """
        base_dir = Path(__file__).parent.parent.parent
        
        if data_dir is None:
            self.data_dir = base_dir / "data" / "classified"
        else:
            self.data_dir = Path(data_dir)
        
        if keywords_path is None:
            keywords_path = base_dir / "configs" / "keywords.json"
        
        # åŠ è½½å…³é”®è¯åº“
        with open(keywords_path, 'r', encoding='utf-8') as f:
            self.keywords = json.load(f)
        
        self.rule_classifier = RuleClassifier(str(keywords_path))
        self.use_llm = use_llm
        self.llm_model = llm_model
        self.top_k = top_k
        
        # LLM å®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._llm_client = None
        
        # æ‰€æœ‰ç±»åˆ«
        self.categories = ["æ—¶æ”¿", "ç»æµ", "å†›äº‹", "ç¤¾ä¼š", "ç§‘æŠ€", "ä½“è‚²", "å¨±ä¹", "å…¶ä»–"]
    
    @property
    def llm_client(self):
        """å»¶è¿ŸåŠ è½½ LLM å®¢æˆ·ç«¯"""
        if self._llm_client is None and self.use_llm:
            try:
                import ollama
                self._llm_client = ollama.Client()
            except ImportError:
                print("è­¦å‘Š: ollama æœªå®‰è£…ï¼Œå°†ç¦ç”¨ LLM åŠŸèƒ½")
                self.use_llm = False
        return self._llm_client
    
    # é¢å¤–çš„é¢†åŸŸå…³é”®è¯æ˜ å°„ï¼ˆè¡¥å……å…³é”®è¯åº“ä¸­æ²¡æœ‰çš„å¸¸è§æŸ¥è¯¢è¯ï¼‰
    DOMAIN_KEYWORDS = {
        "æ—¶æ”¿": ["å¤±ä¸š", "å°±ä¸š", "æ°‘ç”Ÿ", "äººå£", "ç»Ÿè®¡", "æ”¿ç­–", "æ‰¶è´«", "è„±è´«", "ç¤¾ä¿", "å…»è€", 
                 "åŒ»ä¿", "æ•™è‚²", "ä½æˆ¿", "æ°‘æ”¿", "æ•‘åŠ©", "ä½ä¿", "ç–«æƒ…é˜²æ§", "æ”¿åºœå·¥ä½œæŠ¥å‘Š"],
        "ç»æµ": ["GDP", "å¢é•¿", "è‚¡å¸‚", "æˆ¿ä»·", "é€šèƒ€", "åˆ©ç‡", "æ±‡ç‡", "æŠ•èµ„", "æ¶ˆè´¹", "è´¸æ˜“",
                 "è¿›å‡ºå£", "è´¢æ”¿", "ç¨æ”¶", "é‡‘è", "é“¶è¡Œ", "ä¿é™©", "è¯åˆ¸"],
        "å†›äº‹": ["å†›é˜Ÿ", "å›½é˜²", "æ¼”ä¹ ", "æ­¦å™¨", "æˆ˜äº‰", "å†²çª", "å†›äº‹"],
        "ç¤¾ä¼š": ["äº‹æ•…", "çŠ¯ç½ª", "ç¾å®³", "æ•‘æ´", "æ²»å®‰", "å…¬ç›Š", "æ…ˆå–„"],
        "ç§‘æŠ€": ["AI", "äººå·¥æ™ºèƒ½", "5G", "èŠ¯ç‰‡", "ç§‘å­¦", "æŠ€æœ¯", "åˆ›æ–°", "ç ”å‘"],
        "ä½“è‚²": ["æ¯”èµ›", "å† å†›", "ä¸–ç•Œæ¯", "å¥¥è¿", "çƒå‘˜", "è”èµ›"],
        "å¨±ä¹": ["æ˜æ˜Ÿ", "ç”µå½±", "ç”µè§†å‰§", "ç»¼è‰º", "æ­Œæ‰‹", "æ¼”å‘˜"]
    }
    
    def detect_category(self, query: str) -> tuple[str, list[str], float]:
        """
        æ ¹æ®æŸ¥è¯¢è¯æ£€æµ‹åˆ†ç±»
        
        Returns:
            (category, matched_keywords, confidence)
        """
        scores = {cat: 0 for cat in self.categories}
        matched_kws = {cat: [] for cat in self.categories}
        
        query_lower = query.lower()
        
        # 1. é¦–å…ˆæ£€æŸ¥é¢†åŸŸå…³é”®è¯æ˜ å°„
        for category, domain_kws in self.DOMAIN_KEYWORDS.items():
            for kw in domain_kws:
                if kw.lower() in query_lower or kw in query:
                    scores[category] += 2
                    matched_kws[category].append(kw)
        
        # 2. ç„¶åæ£€æŸ¥å…³é”®è¯åº“
        for category, kw_dict in self.keywords.items():
            if category == "å…¶ä»–":
                continue
            
            # å¼ºå…³é”®è¯åŒ¹é… (+3 åˆ†)
            for kw in kw_dict.get("strong", []):
                if kw.lower() in query_lower or kw in query:
                    scores[category] += 3
                    matched_kws[category].append(kw)
            
            # å¼±å…³é”®è¯åŒ¹é… (+1 åˆ†)
            for kw in kw_dict.get("weak", []):
                if kw.lower() in query_lower or kw in query:
                    scores[category] += 1
                    matched_kws[category].append(kw)
        
        # æ‰¾å‡ºæœ€é«˜åˆ†çš„ç±»åˆ«
        best_cat = max(scores, key=scores.get)
        best_score = scores[best_cat]
        
        if best_score == 0:
            return None, [], 0.0  # è¿”å› None è¡¨ç¤ºéœ€è¦è·¨åˆ†ç±»æœç´¢
        
        # è®¡ç®—ç½®ä¿¡åº¦
        total_score = sum(scores.values())
        confidence = best_score / total_score if total_score > 0 else 0
        
        return best_cat, matched_kws[best_cat], confidence
    
    def search(self, query: str, category: str = None, top_k: int = None) -> SearchResponse:
        """
        æ‰§è¡Œæœç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            category: æŒ‡å®šç±»åˆ«ï¼ˆå¯é€‰ï¼Œè‹¥ä¸æŒ‡å®šåˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
            top_k: è¿”å›æ•°é‡ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            SearchResponse å¯¹è±¡
        """
        if top_k is None:
            top_k = self.top_k
        
        # 1. æ£€æµ‹åˆ†ç±»
        cross_category_search = False
        if category is None:
            detected_cat, matched_kws, confidence = self.detect_category(query)
            print(f"ğŸ” æŸ¥è¯¢: {query}")
            
            if detected_cat is None or confidence < 0.3:
                print(f"ğŸ“‚ æ— æ³•ç¡®å®šåˆ†ç±»ï¼Œå°†è·¨æ‰€æœ‰åˆ†ç±»æœç´¢")
                cross_category_search = True
                detected_cat = "å…¨éƒ¨"
            else:
                print(f"ğŸ“‚ è‡ªåŠ¨å®šä½åˆ†ç±»: {detected_cat} (ç½®ä¿¡åº¦: {confidence:.2%})")
                if matched_kws:
                    print(f"ğŸ”‘ åŒ¹é…å…³é”®è¯: {', '.join(matched_kws[:5])}")
        else:
            detected_cat = category
            matched_kws = []
            print(f"ğŸ” æŸ¥è¯¢: {query}")
            print(f"ğŸ“‚ æŒ‡å®šåˆ†ç±»: {detected_cat}")
        
        # 2. åœ¨è¯¥åˆ†ç±»ä¸‹æœç´¢æ–‡æ¡£
        results = self._search_in_category(query, detected_cat, top_k * 2)  # å¤šå–ä¸€äº›ç”¨äºæ’åº
        
        # 3. ä½¿ç”¨ LLM å¢å¼ºæ’åºå’Œæå–ï¼ˆå¯é€‰ï¼‰
        if self.use_llm and results:
            results = self._llm_rerank_and_extract(query, results, top_k)
        
        # 4. å– Top K
        results = results[:top_k]
        
        # 5. æ·»åŠ æ’å
        for i, r in enumerate(results):
            r.rank = i + 1
        
        # 6. ç”Ÿæˆæ€»ç»“ï¼ˆå¯é€‰ï¼‰
        llm_summary = ""
        if self.use_llm and results:
            llm_summary = self._generate_summary(query, results)
        
        return SearchResponse(
            query=query,
            detected_category=detected_cat,
            total_found=len(results),
            results=results,
            llm_summary=llm_summary
        )
    
    def _search_in_category(self, query: str, category: str, limit: int) -> list[SearchResult]:
        """åœ¨æŒ‡å®šåˆ†ç±»ä¸‹æœç´¢æ–‡æ¡£ï¼Œå¦‚æœ category='å…¨éƒ¨' åˆ™è·¨æ‰€æœ‰åˆ†ç±»æœç´¢"""
        results = []
        
        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        query_terms = query.split()
        
        # ç¡®å®šè¦æœç´¢çš„åˆ†ç±»
        if category == "å…¨éƒ¨":
            categories_to_search = self.categories
        else:
            categories_to_search = [category]
        
        for cat in categories_to_search:
            # æ•°æ®ç›®å½•
            cat_dir = self.data_dir / cat
            
            if not cat_dir.exists():
                continue
            
            # éå†è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
            for file_path in cat_dir.glob("*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        doc = json.load(f)
                    
                    # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                    score, matched = self._calculate_relevance(doc, query_terms)
                    
                    if score > 0:
                        results.append(SearchResult(
                            rank=0,
                            title=doc.get("title", "æ— æ ‡é¢˜"),
                            url=doc.get("url", ""),
                            category=cat,
                            snippet=self._extract_snippet(doc, query_terms),
                            relevance_score=score,
                            source_file=str(file_path),
                            matched_keywords=matched
                        ))
                except Exception as e:
                    continue
        
        if not results and category != "å…¨éƒ¨":
            print(f"âš ï¸ åˆ†ç±» '{category}' ä¸‹æœªæ‰¾åˆ°åŒ¹é…æ–‡æ¡£")
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return results[:limit]
    
    def _calculate_relevance(self, doc: dict, query_terms: list) -> tuple[float, list]:
        """è®¡ç®—æ–‡æ¡£ç›¸å…³æ€§åˆ†æ•°"""
        score = 0.0
        matched = []
        
        # åˆå¹¶æ–‡æ¡£æ–‡æœ¬
        title = doc.get("title", "")
        content = doc.get("content", "")
        meta = doc.get("meta_keywords", "") + " " + doc.get("meta_description", "")
        
        text = f"{title} {content} {meta}".lower()
        
        for term in query_terms:
            term_lower = term.lower()
            
            # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
            if term_lower in title.lower():
                score += 5
                matched.append(term)
            # æ­£æ–‡åŒ¹é…
            elif term_lower in text:
                score += 1
                matched.append(term)
        
        # æ‰€æœ‰è¯éƒ½åŒ¹é…åŠ æˆ
        if len(matched) == len(query_terms) and len(query_terms) > 1:
            score *= 1.5
        
        return score, list(set(matched))
    
    def _extract_snippet(self, doc: dict, query_terms: list, max_len: int = 200) -> str:
        """æå–åŒ…å«æŸ¥è¯¢è¯çš„æ‘˜è¦ç‰‡æ®µ"""
        content = doc.get("content", "")
        
        if not content:
            return doc.get("meta_description", "")[:max_len]
        
        # å°è¯•æ‰¾åˆ°åŒ…å«æŸ¥è¯¢è¯çš„æ®µè½
        for term in query_terms:
            pos = content.lower().find(term.lower())
            if pos != -1:
                start = max(0, pos - 50)
                end = min(len(content), pos + max_len - 50)
                snippet = content[start:end]
                if start > 0:
                    snippet = "..." + snippet
                if end < len(content):
                    snippet = snippet + "..."
                return snippet
        
        # æ²¡æ‰¾åˆ°å°±è¿”å›å¼€å¤´
        return content[:max_len] + ("..." if len(content) > max_len else "")
    
    def _llm_rerank_and_extract(self, query: str, results: list, top_k: int) -> list:
        """ä½¿ç”¨ LLM é‡æ–°æ’åºå¹¶æå–å…³é”®ä¿¡æ¯"""
        if not self.llm_client:
            return results
        
        print(f"\nğŸ¤– ä½¿ç”¨ LLM ({self.llm_model}) è¿›è¡Œæ™ºèƒ½æ’åºå’Œä¿¡æ¯æå–...")
        
        # å¯¹ top ç»“æœè¿›è¡Œ LLM è¯„ä¼°
        for result in results[:top_k]:
            try:
                prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼Œå¹¶æå–å…³é”®ä¿¡æ¯ã€‚

æŸ¥è¯¢: {query}

æ–‡æ¡£æ ‡é¢˜: {result.title}
æ–‡æ¡£æ‘˜è¦: {result.snippet}

è¯·è¿”å› JSON æ ¼å¼:
{{"relevance": 0.0-1.0, "key_info": "ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯æ‘˜è¦"}}
"""
                response = self.llm_client.generate(
                    model=self.llm_model,
                    prompt=prompt,
                    options={"temperature": 0.1, "num_predict": 256}
                )
                
                # è§£æå“åº”
                output = response.get('response', '')
                json_match = re.search(r'\{[^{}]*\}', output)
                if json_match:
                    data = json.loads(json_match.group(0))
                    result.relevance_score *= float(data.get("relevance", 0.5))
                    if "key_info" in data:
                        result.snippet = data["key_info"]
            except Exception as e:
                continue
        
        # é‡æ–°æ’åº
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        return results
    
    def _generate_summary(self, query: str, results: list) -> str:
        """ä½¿ç”¨ LLM ç”Ÿæˆæœç´¢ç»“æœæ€»ç»“"""
        if not self.llm_client or not results:
            return ""
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"[{r.rank}] {r.title}\n{r.snippet}"
            for r in results[:5]  # å– top 5 ç”Ÿæˆæ€»ç»“
        ])
        
        prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œå›ç­”ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜ã€‚

ç”¨æˆ·æŸ¥è¯¢: {query}

ç›¸å…³æ–‡æ¡£:
{context}

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¦‚æœä¿¡æ¯ä¸è¶³è¯·è¯´æ˜:
"""
        
        try:
            response = self.llm_client.generate(
                model=self.llm_model,
                prompt=prompt,
                options={"temperature": 0.3, "num_predict": 512}
            )
            return response.get('response', '')
        except Exception as e:
            return f"ç”Ÿæˆæ€»ç»“æ—¶å‡ºé”™: {e}"


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ™ºèƒ½ç½‘é¡µæœç´¢")
    parser.add_argument("query", nargs="?", help="æœç´¢æŸ¥è¯¢")
    parser.add_argument("-c", "--category", help="æŒ‡å®šåˆ†ç±»")
    parser.add_argument("-k", "--top-k", type=int, default=15, help="è¿”å›ç»“æœæ•°é‡")
    parser.add_argument("--no-llm", action="store_true", help="ç¦ç”¨ LLM")
    parser.add_argument("-d", "--data-dir", help="æ•°æ®ç›®å½•")
    parser.add_argument("-m", "--model", default="qwen2:7b", help="LLM æ¨¡å‹")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæœç´¢å™¨
    searcher = SmartSearcher(
        data_dir=args.data_dir,
        use_llm=not args.no_llm,
        llm_model=args.model,
        top_k=args.top_k
    )
    
    # äº¤äº’æ¨¡å¼
    if not args.query:
        print("=" * 60)
        print("ğŸ” æ™ºèƒ½ç½‘é¡µæœç´¢ç³»ç»Ÿ")
        print("=" * 60)
        print("è¾“å…¥æŸ¥è¯¢å†…å®¹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å®šä½åˆ†ç±»å¹¶è¿”å›ç›¸å…³ç»“æœ")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n")
        
        while True:
            try:
                query = input("ğŸ” è¯·è¾“å…¥æŸ¥è¯¢: ").strip()
                if query.lower() in ['quit', 'exit', 'q']:
                    print("å†è§ï¼")
                    break
                if not query:
                    continue
                
                response = searcher.search(query, category=args.category)
                _print_response(response)
                
            except KeyboardInterrupt:
                print("\nå†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
    else:
        response = searcher.search(args.query, category=args.category)
        _print_response(response)


def _print_response(response: SearchResponse):
    """æ‰“å°æœç´¢å“åº”"""
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æœç´¢ç»“æœ (å…±æ‰¾åˆ° {response.total_found} æ¡)")
    print("=" * 60)
    
    if not response.results:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        return
    
    for r in response.results:
        print(f"\nã€{r.rank}ã€‘{r.title}")
        print(f"    ğŸ“‚ åˆ†ç±»: {r.category}")
        print(f"    ğŸ”— URL: {r.url}")
        print(f"    ğŸ“Š ç›¸å…³æ€§: {r.relevance_score:.2f}")
        print(f"    ğŸ“ {r.snippet[:100]}...")
    
    if response.llm_summary:
        print("\n" + "=" * 60)
        print("ğŸ¤– AI æ€»ç»“:")
        print("=" * 60)
        print(response.llm_summary)


if __name__ == "__main__":
    main()
